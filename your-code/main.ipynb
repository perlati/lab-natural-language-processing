{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab | Natural Language Processing\n",
    "### SMS: SPAM or HAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's prepare the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Read Data for the Fraudulent Email Kaggle Challenge\n",
    "- Reduce the training set to speead up development. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DEAR SIR, STRICTLY A PRIVATE BUSINESS PROPOSAL...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Will do.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Nora--Cheryl has emailed dozens of memos about...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dear Sir=2FMadam=2C I know that this proposal ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fyi</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  DEAR SIR, STRICTLY A PRIVATE BUSINESS PROPOSAL...      1\n",
       "1                                           Will do.      0\n",
       "2  Nora--Cheryl has emailed dozens of memos about...      0\n",
       "3  Dear Sir=2FMadam=2C I know that this proposal ...      1\n",
       "4                                                fyi      0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Read Data for the Fraudulent Email Kaggle Challenge\n",
    "data = pd.read_csv(\"/Users/madahbar/Documents/IRONHACK/Week 4/LABS/lab-natural-language-processing/data/kg_train.csv\",encoding='latin-1')\n",
    "\n",
    "# Reduce the training set to speed up development. \n",
    "# Modify for final system\n",
    "data = data.head(1000)\n",
    "print(data.shape)\n",
    "data.fillna(\"\",inplace=True)\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's divide the training and test set into two partitions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 800\n",
      "Validation size: 200\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assume your dataframe is called 'data'\n",
    "# and you have a 'label' column that is 0 (ham) or 1 (spam)\n",
    "X = data['text']          # features\n",
    "y = data['label']         # targets\n",
    "\n",
    "# Split into train and validation sets (80%/20%)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,        # 20% for validation\n",
    "    random_state=42,      # reproducibility\n",
    "    stratify=y            # keep same label distribution\n",
    ")\n",
    "\n",
    "print(\"Train size:\", X_train.shape[0])\n",
    "print(\"Validation size:\", X_val.shape[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, we have to clean the html code removing words\n",
    "\n",
    "- First we remove inline JavaScript/CSS\n",
    "- Then we remove html comments. This has to be done before removing regular tags since comments can contain '>' characters\n",
    "- Next we can remove the remaining tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "def clean_html(text):\n",
    "    # Remove JavaScript/CSS sections\n",
    "    text = re.sub(r'(<script.*?>.*?</script>)|(<style.*?>.*?</style>)', '', text, flags=re.DOTALL)\n",
    "    \n",
    "    # Remove HTML comments\n",
    "    text = re.sub(r'<!--.*?-->', '', text)\n",
    "    \n",
    "    # Remove remaining HTML tags\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    text = soup.get_text()\n",
    "    \n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Remove all the special characters\n",
    "    \n",
    "- Remove numbers\n",
    "    \n",
    "- Remove all single characters\n",
    " \n",
    "- Remove single characters from the start\n",
    "\n",
    "- Substitute multiple spaces with single space\n",
    "\n",
    "- Remove prefixed 'b'\n",
    "\n",
    "- Convert to Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # Remove special characters\n",
    "    text = re.sub(r'\\W', ' ', text)\n",
    "    \n",
    "    # Remove all single characters\n",
    "    text = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', text)\n",
    "    \n",
    "    # Remove single characters from start\n",
    "    text = re.sub(r'\\^[a-zA-Z]\\s+', ' ', text)\n",
    "    \n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Substitute multiple spaces with single space\n",
    "    text = re.sub(r'\\s+', ' ', text, flags=re.I)\n",
    "    \n",
    "    # Remove prefixed 'b' (common from bytes decoding)\n",
    "    text = re.sub(r'^b\\s+', '', text)\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Now let's work on removing stopwords\n",
    "Remove the stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    tokens = text.split()\n",
    "    filtered_words = [word for word in tokens if word not in stop_words]\n",
    "    return ' '.join(filtered_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3b/sqzcq4m91w14zr5xb30wbqnm0000gn/T/ipykernel_31560/2694515195.py:12: MarkupResemblesLocatorWarning: The input passed in on this line looks more like a URL than HTML or XML.\n",
      "\n",
      "If you meant to use Beautiful Soup to parse the web page found at a certain URL, then something has gone wrong. You should use an Python package like 'requests' to fetch the content behind the URL. Once you have the content as a string, you can feed that string into Beautiful Soup.\n",
      "\n",
      "However, if you want to parse some data that happens to look like a URL, then nothing has gone wrong: you are using Beautiful Soup correctly, and this warning is spurious and can be filtered. To make this warning go away, run this code before calling the BeautifulSoup constructor:\n",
      "\n",
      "    from bs4 import MarkupResemblesLocatorWarning\n",
      "    import warnings\n",
      "\n",
      "    warnings.filterwarnings(\"ignore\", category=MarkupResemblesLocatorWarning)\n",
      "    \n",
      "  soup = BeautifulSoup(text, \"html.parser\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DEAR SIR, STRICTLY A PRIVATE BUSINESS PROPOSAL...</td>\n",
       "      <td>dear sir strictly private business proposal mi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Will do.</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Nora--Cheryl has emailed dozens of memos about...</td>\n",
       "      <td>nora cheryl emailed dozens memos haiti weekend...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dear Sir=2FMadam=2C I know that this proposal ...</td>\n",
       "      <td>dear sir fmadam c know proposal might surprise...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fyi</td>\n",
       "      <td>fyi</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  DEAR SIR, STRICTLY A PRIVATE BUSINESS PROPOSAL...   \n",
       "1                                           Will do.   \n",
       "2  Nora--Cheryl has emailed dozens of memos about...   \n",
       "3  Dear Sir=2FMadam=2C I know that this proposal ...   \n",
       "4                                                fyi   \n",
       "\n",
       "                                          clean_text  \n",
       "0  dear sir strictly private business proposal mi...  \n",
       "1                                                     \n",
       "2  nora cheryl emailed dozens memos haiti weekend...  \n",
       "3  dear sir fmadam c know proposal might surprise...  \n",
       "4                                                fyi  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply full cleaning pipeline\n",
    "data['clean_text'] = data['text'].apply(lambda x: clean_html(x))\n",
    "data['clean_text'] = data['clean_text'].apply(lambda x: clean_text(x))\n",
    "data['clean_text'] = data['clean_text'].apply(lambda x: remove_stopwords(x))\n",
    "\n",
    "# Show a preview\n",
    "data[['text', 'clean_text']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tame Your Text with Lemmatization\n",
    "Break sentences into words, then use lemmatization to reduce them to their base form (e.g., \"running\" becomes \"run\"). See how this creates cleaner data for analysis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/madahbar/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/madahbar/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /Users/madahbar/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/madahbar/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/madahbar/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>preprocessed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DEAR SIR, STRICTLY A PRIVATE BUSINESS PROPOSAL...</td>\n",
       "      <td>dear sir strictly private business proposal mi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Will do.</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Nora--Cheryl has emailed dozens of memos about...</td>\n",
       "      <td>nora cheryl email dozen memo haiti weekend ple...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dear Sir=2FMadam=2C I know that this proposal ...</td>\n",
       "      <td>dear sir fmadam c know proposal might surprise...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fyi</td>\n",
       "      <td>fyi</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  DEAR SIR, STRICTLY A PRIVATE BUSINESS PROPOSAL...   \n",
       "1                                           Will do.   \n",
       "2  Nora--Cheryl has emailed dozens of memos about...   \n",
       "3  Dear Sir=2FMadam=2C I know that this proposal ...   \n",
       "4                                                fyi   \n",
       "\n",
       "                                   preprocessed_text  \n",
       "0  dear sir strictly private business proposal mi...  \n",
       "1                                                     \n",
       "2  nora cheryl email dozen memo haiti weekend ple...  \n",
       "3  dear sir fmadam c know proposal might surprise...  \n",
       "4                                                fyi  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If your cleaned text column is called 'clean_text', alias it to match the lab:\n",
    "if 'preprocessed_text' not in data.columns and 'clean_text' in data.columns:\n",
    "    data['preprocessed_text'] = data['clean_text']\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import pos_tag\n",
    "\n",
    "# One-time downloads (safe to run multiple times)\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def _to_wn_tag(treebank_tag):\n",
    "    if treebank_tag.startswith('J'): return wordnet.ADJ\n",
    "    if treebank_tag.startswith('V'): return wordnet.VERB\n",
    "    if treebank_tag.startswith('N'): return wordnet.NOUN\n",
    "    if treebank_tag.startswith('R'): return wordnet.ADV\n",
    "    return wordnet.NOUN  # default\n",
    "\n",
    "def lemmatize_text(s: str) -> str:\n",
    "    tokens = s.split()\n",
    "    tagged = pos_tag(tokens)\n",
    "    lemm = [lemmatizer.lemmatize(tok, _to_wn_tag(tag)) for tok, tag in tagged]\n",
    "    return \" \".join(lemm)\n",
    "\n",
    "# Apply to the whole training dataframe (fast enough for this lab)\n",
    "data['preprocessed_text'] = data['preprocessed_text'].astype(str).apply(lemmatize_text)\n",
    "data[['text','preprocessed_text']].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag Of Words\n",
    "Let's get the 10 top words in ham and spam messages (**EXPLORATORY DATA ANALYSIS**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 HAM words\n",
      " money          981\n",
      "account        907\n",
      "bank           825\n",
      "fund           787\n",
      "transfer       568\n",
      "transaction    551\n",
      "business       514\n",
      "country        508\n",
      "mr             456\n",
      "million        452\n",
      "dtype: int64 \n",
      "\n",
      "Top 10 SPAM words\n",
      " state        136\n",
      "pm           127\n",
      "call         115\n",
      "say          111\n",
      "work         108\n",
      "would        107\n",
      "president     98\n",
      "time          96\n",
      "mr            90\n",
      "get           87\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Separate by class\n",
    "ham_texts  = data.loc[data['label'] == 1, 'preprocessed_text']  # adjust if your lab uses different 0/1 meaning\n",
    "spam_texts = data.loc[data['label'] == 0, 'preprocessed_text']\n",
    "\n",
    "# Vectorize (no stopwords now because we already removed them earlier)\n",
    "cv = CountVectorizer()\n",
    "ham_mat  = cv.fit_transform(ham_texts)\n",
    "ham_vocab = cv.get_feature_names_out()\n",
    "ham_counts = pd.Series(ham_mat.sum(axis=0).A1, index=ham_vocab).sort_values(ascending=False).head(10)\n",
    "\n",
    "# Refit a fresh vectorizer for spam (keeps each class independent for top terms)\n",
    "cv_spam = CountVectorizer()\n",
    "spam_mat  = cv_spam.fit_transform(spam_texts)\n",
    "spam_vocab = cv_spam.get_feature_names_out()\n",
    "spam_counts = pd.Series(spam_mat.sum(axis=0).A1, index=spam_vocab).sort_values(ascending=False).head(10)\n",
    "\n",
    "print(\"Top 10 HAM words\\n\", ham_counts, \"\\n\")\n",
    "print(\"Top 10 SPAM words\\n\", spam_counts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>preprocessed_text</th>\n",
       "      <th>label</th>\n",
       "      <th>money_mark</th>\n",
       "      <th>suspicious_words</th>\n",
       "      <th>text_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>442</th>\n",
       "      <td>Dear=2C Good day hope fine=2Cdear am writting ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>962</th>\n",
       "      <td>FROM MR HENRY KABORETHE CHIEF AUDITOR INCHARGE...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>971</th>\n",
       "      <td>Will do.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>FROM THE DESK OF DR.ADAMU  ISMALERAUDITING AND...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>551</th>\n",
       "      <td>Dear Friend, My name is LOI C.ESTRADA,The wife...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2126</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     preprocessed_text  label  money_mark  \\\n",
       "442  Dear=2C Good day hope fine=2Cdear am writting ...      1           1   \n",
       "962  FROM MR HENRY KABORETHE CHIEF AUDITOR INCHARGE...      1           1   \n",
       "971                                           Will do.      0           0   \n",
       "190  FROM THE DESK OF DR.ADAMU  ISMALERAUDITING AND...      1           1   \n",
       "551  Dear Friend, My name is LOI C.ESTRADA,The wife...      1           1   \n",
       "\n",
       "     suspicious_words  text_len  \n",
       "442                 1      1609  \n",
       "962                 1      3123  \n",
       "971                 0         8  \n",
       "190                 1       530  \n",
       "551                 1      2126  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We add to the original dataframe two additional indicators (money symbols and suspicious words).\n",
    "money_simbol_list = \"|\".join([\"euro\",\"dollar\",\"pound\",\"€\",r\"\\$\"])\n",
    "suspicious_words = \"|\".join([\"free\",\"cheap\",\"sex\",\"money\",\"account\",\"bank\",\"fund\",\"transfer\",\"transaction\",\"win\",\"deposit\",\"password\"])\n",
    "\n",
    "# Create training dataframe with preprocessed text\n",
    "data_train = pd.DataFrame({\n",
    "\t'preprocessed_text': X_train,\n",
    "\t'label': y_train\n",
    "})\n",
    "\n",
    "# Create validation dataframe with preprocessed text  \n",
    "data_val = pd.DataFrame({\n",
    "\t'preprocessed_text': X_val,\n",
    "\t'label': y_val\n",
    "})\n",
    "\n",
    "# Add extra features to training data\n",
    "data_train['money_mark'] = data_train['preprocessed_text'].str.contains(money_simbol_list, case=False, na=False)*1\n",
    "data_train['suspicious_words'] = data_train['preprocessed_text'].str.contains(suspicious_words, case=False, na=False)*1\n",
    "data_train['text_len'] = data_train['preprocessed_text'].apply(lambda x: len(str(x))) \n",
    "\n",
    "# Add extra features to validation data\n",
    "data_val['money_mark'] = data_val['preprocessed_text'].str.contains(money_simbol_list, case=False, na=False)*1\n",
    "data_val['suspicious_words'] = data_val['preprocessed_text'].str.contains(suspicious_words, case=False, na=False)*1\n",
    "data_val['text_len'] = data_val['preprocessed_text'].apply(lambda x: len(str(x))) \n",
    "\n",
    "data_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How would work the Bag of Words with Count Vectorizer concept?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 16792\n",
      "BoW matrix shape: (1000, 16792)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Use the preprocessed text\n",
    "X_text = data['preprocessed_text'].astype(str)\n",
    "\n",
    "# Fit on the whole dataset (or only on train if you already split)\n",
    "cv = CountVectorizer(ngram_range=(1,2), min_df=2)   # unigrams+bigrams, drop ultra-rare terms\n",
    "X_bow = cv.fit_transform(X_text)\n",
    "\n",
    "print(\"Vocabulary size:\", len(cv.vocabulary_))\n",
    "print(\"BoW matrix shape:\", X_bow.shape)   # (n_samples, n_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF\n",
    "\n",
    "- Load the vectorizer\n",
    "\n",
    "- Vectorize all dataset\n",
    "\n",
    "- print the shape of the vetorized dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 16792\n",
      "TF-IDF matrix shape: (1000, 16792)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(ngram_range=(1,2), min_df=2, sublinear_tf=True)\n",
    "X_tfidf = tfidf.fit_transform(data['preprocessed_text'].astype(str))\n",
    "\n",
    "print(\"Vocabulary size:\", len(tfidf.vocabulary_))\n",
    "print(\"TF-IDF matrix shape:\", X_tfidf.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And the Train a Classifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoW + NB accuracy: 0.975\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.96      0.98       112\n",
      "           1       0.96      0.99      0.97        88\n",
      "\n",
      "    accuracy                           0.97       200\n",
      "   macro avg       0.97      0.98      0.97       200\n",
      "weighted avg       0.98      0.97      0.98       200\n",
      "\n",
      "TF-IDF + LR accuracy: 0.985\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "X = data['preprocessed_text'].astype(str)\n",
    "y = data['label']\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Baseline 1: BoW + MultinomialNB\n",
    "bow_nb = Pipeline([\n",
    "    (\"cv\", CountVectorizer(ngram_range=(1,2), min_df=2)),\n",
    "    (\"clf\", MultinomialNB())  # default params per lab\n",
    "])\n",
    "bow_nb.fit(X_train, y_train)\n",
    "pred_nb = bow_nb.predict(X_val)\n",
    "print(\"BoW + NB accuracy:\", accuracy_score(y_val, pred_nb))\n",
    "print(classification_report(y_val, pred_nb))\n",
    "\n",
    "# Baseline 2: TF-IDF + Logistic Regression (optional, strong linear baseline)\n",
    "tfidf_lr = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer(ngram_range=(1,2), min_df=2, sublinear_tf=True)),\n",
    "    (\"clf\", LogisticRegression(max_iter=1000))\n",
    "])\n",
    "tfidf_lr.fit(X_train, y_train)\n",
    "pred_lr = tfidf_lr.predict(X_val)\n",
    "print(\"TF-IDF + LR accuracy:\", accuracy_score(y_val, pred_lr))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Task - Implement a SPAM/HAM classifier\n",
    "\n",
    "https://www.kaggle.com/t/b384e34013d54d238490103bc3c360ce\n",
    "\n",
    "The classifier can not be changed!!! It must be the MultinimialNB with default parameters!\n",
    "\n",
    "Your task is to **find the most relevant features**.\n",
    "\n",
    "For example, you can test the following options and check which of them performs better:\n",
    "- Using \"Bag of Words\" only\n",
    "- Using \"TF-IDF\" only\n",
    "- Bag of Words + extra flags (money_mark, suspicious_words, text_len)\n",
    "- TF-IDF + extra flags\n",
    "\n",
    "\n",
    "You can work with teams of two persons (recommended)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(A) BoW + NB:       0.9750\n",
      "(B) TF-IDF + NB:    0.9750\n",
      "(C) TF-IDF + flags: 0.9750\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Ensure extra features exist\n",
    "if not {'money_mark','suspicious_words','text_len'}.issubset(data.columns):\n",
    "    # quick creation (same as earlier cell)\n",
    "    money_simbol_list = r\"|\".join([\"euro\",\"dollar\",\"pound\",\"€\",\"£\",\"$\"])\n",
    "    suspicious_words = r\"|\".join([\"free\",\"cheap\",\"sex\",\"money\",\"account\",\"bank\",\"fund\",\"transfer\",\"transaction\",\"win\",\"deposit\",\"password\"])\n",
    "    data['money_mark']      = data['preprocessed_text'].str.contains(money_simbol_list, regex=True).astype(int)\n",
    "    data['suspicious_words']= data['preprocessed_text'].str.contains(suspicious_words, regex=True).astype(int)\n",
    "    data['text_len']        = data['preprocessed_text'].apply(len)\n",
    "\n",
    "# Split once so all experiments are comparable\n",
    "X_train_txt, X_val_txt, y_train, y_val = train_test_split(\n",
    "    data['preprocessed_text'].astype(str), data['label'],\n",
    "    test_size=0.2, random_state=42, stratify=data['label']\n",
    ")\n",
    "\n",
    "# ----- (A) BoW only -----\n",
    "cv = CountVectorizer(ngram_range=(1,2), min_df=2)\n",
    "Xtr_bow = cv.fit_transform(X_train_txt)\n",
    "Xva_bow = cv.transform(X_val_txt)\n",
    "\n",
    "clf_nb = MultinomialNB()\n",
    "clf_nb.fit(Xtr_bow, y_train)\n",
    "pred_A = clf_nb.predict(Xva_bow)\n",
    "\n",
    "# ----- (B) TF-IDF only -----\n",
    "tfidf = TfidfVectorizer(ngram_range=(1,2), min_df=2, sublinear_tf=True)\n",
    "Xtr_tfidf = tfidf.fit_transform(X_train_txt)\n",
    "Xva_tfidf = tfidf.transform(X_val_txt)\n",
    "\n",
    "clf_nb_B = MultinomialNB()\n",
    "clf_nb_B.fit(Xtr_tfidf, y_train)\n",
    "pred_B = clf_nb_B.predict(Xva_tfidf)\n",
    "\n",
    "# ----- (C) TF-IDF + extra flags -----\n",
    "# Build extra features matrices aligned to the same splits\n",
    "extra_train = data.loc[X_train_txt.index, ['money_mark','suspicious_words','text_len']].copy()\n",
    "extra_val   = data.loc[X_val_txt.index,   ['money_mark','suspicious_words','text_len']].copy()\n",
    "\n",
    "# Scale numeric extras (NB expects non-negative; StandardScaler(with_mean=False) keeps sparsity safe)\n",
    "scaler = StandardScaler(with_mean=False)\n",
    "extra_train_scaled = scaler.fit_transform(extra_train)\n",
    "extra_val_scaled   = scaler.transform(extra_val)\n",
    "\n",
    "Xtr_combo = hstack([Xtr_tfidf, extra_train_scaled])\n",
    "Xva_combo = hstack([Xva_tfidf, extra_val_scaled])\n",
    "\n",
    "clf_nb_C = MultinomialNB()\n",
    "clf_nb_C.fit(Xtr_combo, y_train)\n",
    "pred_C = clf_nb_C.predict(Xva_combo)\n",
    "\n",
    "# -------- Results ----------\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(f\"(A) BoW + NB:       {accuracy_score(y_val, pred_A):.4f}\")\n",
    "print(f\"(B) TF-IDF + NB:    {accuracy_score(y_val, pred_B):.4f}\")\n",
    "print(f\"(C) TF-IDF + flags: {accuracy_score(y_val, pred_C):.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
